# 一些数学知识点整理

### 特征值分解

* 定义：A为n阶方阵，若存在数*λ*及非零n维列向量*α*使得$Aα = λα$，则称*λ*为A的一个特征值，*α*是矩阵A属于特征值*λ*的一个特征向量，$|λE - A|$为矩阵A的特征多项式，$|λE - A| = 0$称为A的特征方程。

* 求法：先通过$|λE - A| = 0$求出矩阵A的特征值$λ_i$，再由$(λ_iE - A)x = 0$求基础解系，即为属于特征值$λ_i$的特征向量。

* 设A和B都是n阶矩阵，若存在可逆矩阵P使得$P^{-1}AP = B$，则称矩阵A和B相似，记作A~B。
* 如果A能与对角矩阵相似，则称A可对角化。
* 所有属于同一特征值的特征向量的非零线性组合仍然是属于该特征值的特征向量。
* 属于不同特征值的特征向量的线性组合，不是原矩阵的特征向量。
* 所有特征值之和等于矩阵的迹，所有特征值之积等于矩阵行列式的值。
* 属于不同特征值的特征向量构成的向量组线性无关。
* 属于m重特征值的线性无关的特征向量的个数不超过m个。
* 若n阶矩阵A和B相似，则A与B由相同的特征多项式和特征值。即若A~B，则$|λE - A| = |λE - B|$。
* n阶方阵A可对角化的充要条件是A有n个线性无关的特征向量。

### 奇异值分解

* $A = U∑V^{T}$，A(m×n)，U(m×m)（左奇异向量，两两正交），∑(n×n)（对角矩阵，对角线上元素为奇异值），V(n×n)（右奇异向量，两两正交）

* 将奇异值从大到小排列，由此∑便能由M唯一确定。
* 很多情况下，前10%甚至1%的奇异值的和就占了全部奇异值的99%以上，因此可以取前r大的奇异值进行部分奇异值分解：$A_{m×n} ≈ U_{m×r}∑_{r×r}V^{T}_{r×n}$，r是一个远小于m、n的数。r越接近n，相乘的结果越接近A，达到了压缩空间和降维的目的。具体在PCA方法中有应用：基于特征值分解/奇异值分解协方差矩阵实现PCA算法。

> 奇异值分解能简约数据，去除噪声和冗余数据，是一种降维方法，将数据映射到低维空间. 它的数学原理是矩阵分解的方法.
>
> 奇异值分解能用于任意大小的矩阵，而特征分解只能用于特定类型的方阵，因而奇异值分解的适用范围更广.

* [主成分分析（PCA）原理详解](https://blog.csdn.net/program_developer/article/details/80632779)