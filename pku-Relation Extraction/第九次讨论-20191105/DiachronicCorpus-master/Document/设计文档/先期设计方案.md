# Diachronic Corpus设计方案

## 目录

[TOC]

## 需求分析

### 概要

历时语料库是在普通的语料库的基础上，加入时间轴，在时间维度上对语料进行划分并分析的语料库。一方面要面向数据库维护人员，方便进行数据的导入、预处理和管理工作；另一方面要面向研究者，提供基本的数据统计和分析，以及下载功能。因此，其主要需求为以下三点：

1. 对大规模非结构化数据进行有效、稳定的预处理和存储。
2. 支持高性能、多维度检索，对检索结果的统计、展示和下载等功能。
3. 支持后续科研成果向平台功能的转化。

### 用况分析

#### 数据库操作人员

1. 功能需求：非结构化数据预处理
   性能需求：保证健壮性和吞吐量
2. 功能需求：海量数据存储
   性能需求：总量在TB级，峰值速率的压力可能不会很大

#### 科研用户

1. 功能需求：数据库的实时查询和结果统计分析
   性能需求：对简单的查询需求，响应速度达到秒级；大规模数据的导出和组合查询，响应速度达到分钟级
2. 功能需求：查询结果的图形化展示和管理
3. 功能需求：在语料上进行多种机器学习算法的实现

#### ？：我们要为语言学的研究者考虑用况吗？

## 相关工作

语料库方面，主要对北京语言大学语料库中心、北京大学中国语言学研究中心、英国国家语料库进行了调研。

算法方面主要对Text Cube的相关算法进行了调研。

### 北京语言大学语料库中心

“北京语言大学语料库中心(BCC)”是以汉语为主、兼有其他语种的语言大数据，目标是为语言本体研究提供一个使用简便的在线检索系统和构建大数据的语言应用基础平台。BCC 支持云服务，通过 API 调用方式为开展知识抽取、模型构建等研究和应用工作提供便利。

主要包括三方面工作:语料库资源建设、检索引擎开发和提供语料库检索服务。如图 1 所示，语料库的资源建设是构建语料库数据内容的基础。BCC 主要包括三种类型语料:多语种单语语料库、双语对齐语料库和深加工的树库。语料库检索内核是实现语料库系统的技术基础，采用基于后缀串的全文检索算法，并且支持通配符和离合模式匹配。检索服务是指使用语料库系统的方式和方法。

总字数约 150 亿字，包括：报刊（20 亿）、文学（30 亿）、微博（30 亿）、科技（30 亿）、综合（10 亿）和古汉语（20 亿）等多领域语料，是可以全面反映当今社会语言生活的大规模语料库。![屏幕快照 2018-01-15 下午3.27.14](/Users/wuxian/Documents/assignment/网络大数据/final/屏幕快照 2018-01-15 下午3.27.14.png)

* 语料库资源建设
  * 语料库涵盖多语种
  * 多层次语料加工
  * 现代古汉语语料和古代汉语语料兼具
  * 汉语多语体（小说、报纸，甚至微博）
  * 共时语料和历时语料兼具
* 语料库采集加工平台

  * 网上语料采集工具
  * 语料整理工具
  * 语言自动分析工具
  * 语料库标注平台
* 检索引擎
  * 支持语言大数据
  * 支持多语种检索
  * 支持多语料检索
  * 支持复杂检索

此外，还有关于词典的功能：

​	汉字字频、字释义

​	搭配扩展：左右邻词（按词性）

​	义项频率

在加工流程方面，我们可以借鉴入预处理环节：

![屏幕快照 2018-01-19 下午8.40.54](/Users/wuxian/Desktop/屏幕快照 2018-01-19 下午8.40.54.png)

BCC给人最直接的使用感受是搜索功能很强大，具备词性、通配符、词语混合搜索的功能。语料库相比另外两个也显得覆盖面更广。

### 北京大学中国语言学研究中心

CCL的语料规模相比BCC小了很多，大约是7亿字；但时间跨度非常大，从公元前11世纪一直到当代的语料都有涉及。而且具有双语对齐的语料，句对数23万，中文字数600万左右。

功能上也是以支持复杂的检索表达式为特色：

- 支持复杂检索表达式（比如不相邻关键字查询，指定距离 查询，等等）；
- 支持对标点符号的查询（比如查询“？”可以检索语料库 中所有疑问句）；
- 支持在“结果集”中继续检索；
- 用户可定制查询结果的显示方式（如左右长度，排序 等）；
- 用户可从网页上下载查询结果(text文件)；

其中的模式查询基本具备了BCC的的复杂检索的功能，而且支持更多检索模式。

遗憾的是虽然带有这么大的时间跨度，CCL只是将语料划分成了“现代汉语”和“古代汉语”两个大类，分别在各自的范围进行检索，没有在时间轴上的统计，并不能为我们提供历时语料库的功能范例。

另外双语语料库的部分，也是基于上述检索功能存在的。只是展示的时候分列了中文和英语的句对。

总的来说是个很纯粹的语料库。

### 英国国家语料库

英国国家语料库收录了来自各种来源的书面和口头语言样本，旨在代表20世纪后期英国英语的广泛的横截面，包括口头和口头语言书面。最新版本是2007年发布的BNC XML版本。

BNC的书面部分占90%，包括地区和国家报纸，覆盖全年龄和兴趣的专业期刊和期刊，学术书籍和流行小说，已发表和未发表的信件和备忘录，学校和大学散文摘录等等。口语部分（10％）由非正式的非正式交谈（由不同年龄，地区和社会阶层的志愿者以人口均衡的方式记录）和不同背景下收集的口语进行转写，包括从正式的商业或政府会议到广播节目和电话通话。

它的主要特点是：

* 单语：它只涉及现代英式英语，不过也有一些别的语言在语料中。
* 共时：它涵盖二十世纪后期的英国英语，而不是产生它的历史发展。
* 一般性：它包括许多不同的风格和品种，并不限于任何特定的主题领域，流派或注册。特别是它还包含口头和书面语言。
* 采样：对于书面来源，从单作者的文本中采样了45,000字的样本。字数在45,000字以内的较短文本，或杂志和报纸等多作者文本，是完全包含的。抽样可以在1亿个单词的限制范围内扩大文本的覆盖范围，避免过度表现特殊的文本。

BNC干脆就直接提供的是语料下载，没提供检索之类的功能。不过它在有限的容量内提供尽可能丰富全面的信息的思路值得借鉴。

### Text Cube

*Text Cube: Computing IR Measures for Multidimensional Text Database Analysis*

TODO：补完

## 历时语料库平台设计概要

### 设计思路和功能

三个语料库的设计和形成时间越来越早（不过顺序反过来调研就是形成时间越来越晚，距离现在越来越近了），可以看到随着时间的推进，语料库的功能也越来越丰富。除了BNC之外，CCL和BCC都具有强大的检索功能，这是大规模语料库便于使用的基础。BNC的设计思路就是小规模但尽可能覆盖广泛的语言资源，不提供检索也罢。但是在CCL和BCC这种10亿至100亿级字数的语料库上，检索是必须具备的能力。

但是只有检索，我们的语料库就体现不出亮点和必要性，特别是从语料的规模上，目前看我们也没什么优势。就像一开始说的，我们要具有更丰富的可以为研究提供便利的特征加入进去。

从已有的研究，比如词义变迁（时间角度）和多义词/同义词（空间角度），可以尝试将“词”扩展到“义项”的级别为语料库建立索引。

从正在进行的研究，句子的句法构件和整个句子的模式都是可以提取出的特征。句法构件是我想用来解决句子转述评价的G差异的一个概念。句法构件是从句法树和依存树提取出的子结构，搭配构成句子的词语，尝试从句子中提取出类似于“把字句”、“兼语句”、“状补结构”这种东西。希望用这个东西辅助解决G差异的自动判定，和句子特征的建模。

基于上述所有可以提取出的特征，将我们语料库中所有的句子扩充成<feature1, feature2, …, doc>的cell，建立基于其的检索系统，并提供检索出的语料的下载和简要分析。

### 平台框架设计

从需求出发，历时语料库管理分析平台是包含从数据预处理、存储、统计分析，直到检索和对结果的分析的完整平台。基于大规模分布式文件系统，并且提供足够的运算能力和检索能力。整个平台的架构如下：

![dc_arch](/Users/wuxian/Documents/assignment/网络大数据/final/dc_arch.png)

平台共分为5个模块，分别是数据预处理层、数据存储层、数据存储层、数据计算、检索与统计分析模块和自定义算法模块。管理员负责整个架构的开发和维护，科研用户利用基于数据计算层的检索与统计分析结果进行研究，而实验室的同学们可以利用研究成果，将算法加入自定义算法模块，甚至固化到数据计算层，提供更丰富的检索方式。

平台旨在提供大规模语料与多维检索的解决方案，并尽可能丰富实验室成果对外的展示能力。通过加入对第三方开放的自定义算法模块，集合更多科研用户的智慧。

将数据预处理层和数据计算层解耦。数据计算层与数据预处理层分属存储层的两端，预处理关注的是从非结构化文本中提取出结构化存储所需要的特征，这个过程可以看成是动态的；经过存储层落地成静态数据；而在计算层，才能在静态数据上进行传统的数据挖掘工作。

## 模块设计

### 数据预处理层

至少截止我做毕设时的ACL Anthology，当时的系统依然不能做到消息队列式的预处理流程。但是在这学期初的第一次组会时，我研一这年的一个任务是预处理的流程化，包含下载、转换、清洗，标题正文引文提取，分词、sense tagging。

我倾向于使用消息队列来维护预处理流程，因为之前做的uniform的流程一旦中间出点问题就很难追踪和补救了。大致思路是把预处理流程分成几个小模块，由主进程抛出预处理任务，预处理流程模块监听消息队列并进行处理，在末尾流入存储层落地。比较简单的消息队列可以考虑redis，如果写的工业化一点可以考虑Kafka，因为毕竟redis只是“能”做消息队列而已。

### 数据存储层

数据存储层目前使用的是mongoDB对数据进行存储。从上个月完成的简易demo上看，仅有mongoDB提供的索引能力有点弱（太弱了），目前还能勉强满足检索需求（实际上只有词索引，还得自己预先做分词），如果后续增加更多检索方式，肯定要拆分到下面的检索和统计分析模块进行，自己设计后缀树或者其他结构了。

对于语料的存储，目前还是将原始语料存储在了mongoDB中。如果进一步扩大可能会考虑将特征和元信息保留在数据库中，而原始语料放在基于Lucene的Elastic Search提高存储和检索性能。

### 数据计算层

这里需要提供的是基于存储层模型的高性能运算框架。毕竟就算特征提的再好，算不出来或者索引不出来都是假的。最通用而且简易的框架就是map reduce了吧，这学期上课用的时候还是深切体会到这个框架的方便和强大的，而且mongoDB本身也提供了map reduce支持。

### 检索和统计分析模块

检索项除了传统的词、词性、时间，参考[历时语料库平台设计概要/设计思路和功能](#历时语料库平台设计概要/设计思路和功能)加入更多索引项。

检索模块使用Lucene和ElasticSearch进行全文和索引检索。

统计分析模块将检索结果进行初步的分析并以表格、图表等方式进行展示，这个BCC做的很实用，可以参考。