## 自监督学习 Self-Supervised Learning

* The machine predicts any part of its input for many observed part.
* **A lot of feedback**

- 自监督学习是监督学习的一个特例，它与众不同，值得单独分为一类。自监督学习是**没有人工标注标签的监督学习**，可以将它看作没有人类参与的监督学习。**标签仍然存在**（因为总要有什么东西来监督学习过程），但它们是**从输入数据中生成的**，通常使用启发式算法生成的。



- autonomous supervised learning

- 是一种表示学习方法，不需要预先已经标注好的数据，只需要上下文和embedded metadata 作为监督信号。

  就图像识别来说，可以通过图像上的相对位置信息作为监督信号，训练得到丰富的视觉表示，识别出该图像的内容是什么。不仅使用于计算机视觉，还有其他领域。

#### Self-supervised vs. supervised learning

自监督是监督学习，因为它们的目标都是从数据对（输入和有标签的输出）学得一个函数。但自监督并不是像监督学习那样需要明显的带标签的输入输出数据对，而是把correlations, embedded metadata, or domain knowledge available （输入中隐含的或者从数据中自动抽取）作为监督信号。自监督学习也已经能用于回归和分类啦。

#### Self-supervised vs. unsupervised learning

​	自监督类似无监督学习，因为都是从没有明确标签的数据中进行学习。无监督学习是学习数据的内在关系、结构，着重于clustering、grouping、dimensionality reduction, recommendation engines, density estimation, or anomaly detection，这些都与自监督不一样。

#### Self-Supervised vs. semi-supervised learning

半监督学习是使用小部分有标签数据、大部分无标签数据进行学习；但自监督学习使用的数据都是**没有明确提供标签的数据**。

针对监督学习的缺点，学习方法和scalability，即需要大量的有标签数据、数据清洗、为某些特定问题专门训练一个模型，这与人类的学习方式不一样。人们花了多年时间在数据收集和专业标注上。而人类在学习的时候，需要少量数据、多源的，可以针对多个任务，能很好的泛化。



### Pre-Training

个人认为，之所以用Pre-Train，主要有两点原因：

* ATOMIC的常识数据比较少（有待考证），因此先用GPT训练好的模型参数初始化，再用少量数据使其迁移到Generating Commonsense任务上。
* GPT任务和Commonsense任务对网络需要学到的信息比较相近，先用其参数初始化后fine-tuning即可使网络获得Generating Commonsense的能力。





个人理解，自监督学习是没有label的，但是是通过输入数据按某些规则自己生成一些label作为参照（比如snowball里面的pattern，是通过seed set去和语料库按模式匹配而生成的，反过来又用它来生成新的seed）。



传统的监督学习：需要大量labeled data，人工标注数据费时费力且昂贵

Self-Supervised：提出pretext tasks，在训练Objective functions的过程中学到features。

> the networks can be trained by learning objective functions of the pretext tasks and the features are learned through this process.

在训练的过程当中自动生成“伪标签”。

自监督学习：使用无标签的数据，通过自动生成的标签来训练（比如旋转图片，以旋转角度为标签；或snowball中的设计一种vector格式，以句子相应的vector作为标签），学到数据的feature后，迁移到其他任务上。



